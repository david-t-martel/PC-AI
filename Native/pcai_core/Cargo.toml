[workspace]
resolver = "2"
members = [
    "pcai_core_lib",
    "pcai_inference",
]

[workspace.package]
version = "0.1.0"
authors = ["David Martel"]
edition = "2021"
license = "MIT"
repository = "https://github.com/david-t-martel/PC_AI"

[workspace.dependencies]
# FFI and C interop
libc = "0.2"

# Parallel processing
rayon = "1.10"

# High-performance file walking (ripgrep's engine)
ignore = "0.4"

# SHA-256 hashing
sha2 = "0.10"

# Glob pattern matching
globset = "0.4"

# Regular expressions
regex = "1.11"

# JSON serialization
serde = { version = "1.0.217", features = ["derive"] }
serde_json = "1.0.149"

# Windows API bindings
windows-sys = "0.59"

# UTF-16 strings for Windows APIs
widestring = "1.1"

# System information (cross-platform)
sysinfo = "0.38"

# Directory walking
walkdir = "2.5"

# Concurrency primitives
parking_lot = "0.12"

# Windows Registry access
winreg = "0.55"

# Utility dependencies
path-slash = "0.2"
dunce = "1.0"
anyhow = "1.0.100"
thiserror = "2.0"
tokio = { version = "1.41", features = ["full"] }
reqwest = { version = "0.12", features = ["json", "blocking"] }
base64 = "0.22"
hex = "0.4"
memmap2 = "0.9.5"
num_cpus = "1.16"
chrono = { version = "0.4", features = ["serde"] }
log = "0.4"
env_logger = "0.11"

# Observability
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# Async helpers
async-trait = "0.1"
futures = "0.3"

# IDs
uuid = { version = "1.11", features = ["v4"] }

# HTTP server
axum = "0.7"
tower-http = { version = "0.6", features = ["trace", "cors"] }

# LLM backends
llama-cpp-2 = { version = "0.1", features = ["sampler"] }
encoding_rs = "0.8"
mistralrs = "0.7"
mistralrs-core = "0.7"

[profile.release]
opt-level = 3      # Maximum optimization
lto = false        # Disable LTO for faster troubleshooting
codegen-units = 16 # Parallelize for faster build
strip = false      # Keep symbols for verification
panic = "abort"    # Smaller binary, faster panics

[patch.crates-io]
candle-kernels = { path = "vendor/candle-kernels-0.9.2" }
