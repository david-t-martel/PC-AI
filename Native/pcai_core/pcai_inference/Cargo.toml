[package]
name = "pcai-inference"
version = "0.1.0"
edition = "2021"
authors = ["PC_AI Team"]
description = "Dual-backend LLM inference engine for PC diagnostics (llama.cpp and mistral.rs)"
license = "MIT"

[lib]
name = "pcai_inference_lib"
crate-type = ["cdylib", "rlib"]

[dependencies]
# Core dependencies
anyhow.workspace = true
thiserror.workspace = true
tokio.workspace = true
serde.workspace = true
serde_json.workspace = true
tracing.workspace = true
tracing-subscriber.workspace = true
async-trait.workspace = true
uuid.workspace = true
futures.workspace = true
reqwest.workspace = true

# HTTP server (optional)
axum = { workspace = true, optional = true }
tower-http = { workspace = true, optional = true }

# Backend: llama.cpp (optional) - CUDA disabled by default for CPU-only builds
llama-cpp-2 = { workspace = true, optional = true }
encoding_rs = { workspace = true, optional = true }

# Backend: mistral.rs (optional)
# Switched from local path to external crate as per USER directive
mistralrs = { workspace = true, optional = true }
mistralrs-core = { workspace = true, optional = true }

[dev-dependencies]
tempfile = "3.14"

[features]
default = ["server"]
llamacpp = ["dep:llama-cpp-2", "dep:encoding_rs"]
mistralrs-backend = ["dep:mistralrs", "dep:mistralrs-core"]
# CUDA feature gates GPU acceleration for backends
cuda-llamacpp = ["llama-cpp-2/cuda"]
cuda-mistralrs = ["mistralrs/cuda", "mistralrs-core/cuda"]
# Back-compat umbrella feature
cuda = ["cuda-llamacpp", "cuda-mistralrs"]
server = ["dep:axum", "dep:tower-http"]
ffi = []

[build-dependencies]
chrono = "0.4"

[[bin]]
name = "pcai-llamacpp"
path = "src/bin/llamacpp.rs"
required-features = ["llamacpp", "server"]

[[bin]]
name = "pcai-mistralrs"
path = "src/bin/mistralrs.rs"
required-features = ["mistralrs-backend", "server"]
