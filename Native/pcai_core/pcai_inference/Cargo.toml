[package]
name = "pcai-inference"
version = "0.1.0"
edition = "2021"
authors = ["PC_AI Team"]
description = "Dual-backend LLM inference engine for PC diagnostics (llama.cpp and mistral.rs)"
license = "MIT"

[lib]
crate-type = ["cdylib", "rlib"]

[dependencies]
# Core dependencies
anyhow = "1.0"
thiserror = "2.0"
tokio = { version = "1.41", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
async-trait = "0.1"
uuid = { version = "1.11", features = ["v4"] }
futures = "0.3"
reqwest = { version = "0.12", features = ["json"] }

# HTTP server (optional)
axum = { version = "0.7", optional = true }
tower-http = { version = "0.6", features = ["trace", "cors"], optional = true }

# Backend: llama.cpp (optional) - CUDA disabled by default for CPU-only builds
llama-cpp-2 = { version = "0.1", optional = true, features = ["sampler"] }
encoding_rs = { version = "0.8", optional = true }

# Backend: mistral.rs (optional)
# Using local fork with CUDA 13.0 improvements (CPU-only on Windows due to bindgen_cuda issues)
mistralrs = { path = "T:/projects/rust-mistral/mistral.rs/mistralrs", optional = true }
mistralrs-core = { path = "T:/projects/rust-mistral/mistral.rs/mistralrs-core", optional = true }

[dev-dependencies]
tempfile = "3.14"

[features]
default = ["llamacpp", "server"]
llamacpp = ["dep:llama-cpp-2", "dep:encoding_rs"]
mistralrs-backend = ["dep:mistralrs", "dep:mistralrs-core"]
# CUDA feature gates GPU acceleration for backends
cuda = ["llama-cpp-2/cuda", "mistralrs/cuda", "mistralrs-core/cuda"]
server = ["dep:axum", "dep:tower-http"]
ffi = []

[build-dependencies]

[[bin]]
name = "pcai-inference"
path = "src/main.rs"
required-features = ["server"]
