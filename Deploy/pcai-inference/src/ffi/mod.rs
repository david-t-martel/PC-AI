//! FFI exports for PowerShell integration
//!
//! This module provides a C-compatible FFI interface for calling the pcai-inference
//! library from PowerShell via P/Invoke.
//!
//! # Safety
//!
//! All FFI functions accept raw pointers from C callers. The safety requirements are
//! documented on each function. This module allows `clippy::not_unsafe_ptr_arg_deref`
//! because marking FFI functions as `unsafe` doesn't help C/C#/PowerShell callers
//! who cannot see Rust's `unsafe` keyword.

#![allow(clippy::not_unsafe_ptr_arg_deref)]
//!
//! ## Thread Safety
//!
//! - Global state is protected by Mutex
//! - Runtime is created once and reused
//! - Errors are stored in thread-local storage
//!
//! ## Usage from PowerShell
//!
//! ```powershell
//! # Load the DLL
//! Add-Type -Path "pcai_inference.dll" -Namespace PCAI -Name Inference
//!
//! # Initialize with backend
//! [PCAI.Inference]::pcai_init("llamacpp")
//!
//! # Load model
//! [PCAI.Inference]::pcai_load_model("path/to/model.gguf", -1)
//!
//! # Generate text
//! $result = [PCAI.Inference]::pcai_generate("Hello", 100, 0.7)
//! $text = [System.Runtime.InteropServices.Marshal]::PtrToStringAnsi($result)
//! [PCAI.Inference]::pcai_free_string($result)
//! ```

use std::cell::RefCell;
use std::ffi::{CStr, CString};
use std::os::raw::{c_char, c_void};
use std::sync::{Mutex, OnceLock};

use tokio::runtime::Runtime;

use crate::backends::{GenerateRequest, InferenceBackend};
#[cfg(any(feature = "llamacpp", feature = "mistralrs-backend"))]
use crate::backends::BackendType;
use crate::Error;

// ============================================================================
// Error Codes
// ============================================================================

/// Error codes returned by FFI functions
#[repr(i32)]
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum PcaiErrorCode {
    /// Operation succeeded
    Success = 0,
    /// Backend not initialized (call pcai_init first)
    NotInitialized = -1,
    /// Model not loaded (call pcai_load_model first)
    ModelNotLoaded = -2,
    /// Invalid input parameter (null pointer, invalid UTF-8, etc.)
    InvalidInput = -3,
    /// Backend operation failed
    BackendError = -4,
    /// I/O error (file not found, permission denied, etc.)
    IoError = -5,
    /// Unknown or unclassified error
    Unknown = -99,
}

impl PcaiErrorCode {
    /// Convert from i32 to PcaiErrorCode
    pub fn from_i32(code: i32) -> Self {
        match code {
            0 => Self::Success,
            -1 => Self::NotInitialized,
            -2 => Self::ModelNotLoaded,
            -3 => Self::InvalidInput,
            -4 => Self::BackendError,
            -5 => Self::IoError,
            _ => Self::Unknown,
        }
    }
}

// ============================================================================
// Global State
// ============================================================================

/// Global state holding the runtime and backend
struct GlobalState {
    runtime: Runtime,
    backend: Option<Box<dyn InferenceBackend>>,
}

/// Global state instance (initialized once)
static GLOBAL_STATE: OnceLock<Mutex<GlobalState>> = OnceLock::new();

// Thread-local error storage
thread_local! {
    static LAST_ERROR: RefCell<Option<String>> = const { RefCell::new(None) };
    static LAST_ERROR_CODE: RefCell<PcaiErrorCode> = const { RefCell::new(PcaiErrorCode::Success) };
}

/// Initialize global state on first access
fn init_global_state() -> &'static Mutex<GlobalState> {
    GLOBAL_STATE.get_or_init(|| {
        let runtime = Runtime::new().expect("Failed to create Tokio runtime");
        Mutex::new(GlobalState {
            runtime,
            backend: None,
        })
    })
}

/// Set the last error with a specific error code
fn set_last_error_with_code(err: impl Into<String>, code: PcaiErrorCode) {
    LAST_ERROR.with(|e| {
        *e.borrow_mut() = Some(err.into());
    });
    LAST_ERROR_CODE.with(|c| {
        *c.borrow_mut() = code;
    });
}

/// Clear the last error for the current thread
fn clear_last_error() {
    LAST_ERROR.with(|e| {
        *e.borrow_mut() = None;
    });
    LAST_ERROR_CODE.with(|c| {
        *c.borrow_mut() = PcaiErrorCode::Success;
    });
}

/// Get a C string from a raw pointer, handling null safely
unsafe fn c_str_from_ptr<'a>(ptr: *const c_char) -> Result<&'a str, Error> {
    if ptr.is_null() {
        return Err(Error::InvalidInput("Null pointer".to_string()));
    }
    CStr::from_ptr(ptr)
        .to_str()
        .map_err(|e| Error::InvalidInput(format!("Invalid UTF-8: {}", e)))
}

// ============================================================================
// FFI Exports
// ============================================================================

/// Initialize the inference backend
///
/// # Arguments
/// * `backend_name` - Backend to use: "llamacpp" or "mistralrs"
///
/// # Returns
/// * 0 on success
/// * -1 on error (check pcai_last_error)
///
/// # Safety
/// * `backend_name` must be a valid null-terminated C string
/// * Must be called before any other functions except pcai_last_error
#[no_mangle]
pub extern "C" fn pcai_init(backend_name: *const c_char) -> i32 {
    clear_last_error();

    // Parse backend name
    let backend_str = match unsafe { c_str_from_ptr(backend_name) } {
        Ok(s) => s,
        Err(e) => {
            set_last_error_with_code(
                format!("Invalid backend name: {}", e),
                PcaiErrorCode::InvalidInput,
            );
            return PcaiErrorCode::InvalidInput as i32;
        }
    };

    // Determine backend type
    #[cfg(not(any(feature = "llamacpp", feature = "mistralrs-backend")))]
    {
        set_last_error_with_code(
            format!("Unknown backend: {}. No backend features enabled. Enable 'llamacpp' or 'mistralrs-backend' feature.", backend_str),
            PcaiErrorCode::InvalidInput,
        );
        PcaiErrorCode::InvalidInput as i32
    }

    #[cfg(any(feature = "llamacpp", feature = "mistralrs-backend"))]
    let backend_type: BackendType = match backend_str.to_lowercase().as_str() {
        #[cfg(feature = "llamacpp")]
        "llamacpp" | "llama.cpp" | "llama_cpp" => BackendType::LlamaCpp,

        #[cfg(feature = "mistralrs-backend")]
        "mistralrs" | "mistral.rs" | "mistral_rs" => BackendType::MistralRs,

        _ => {
            set_last_error_with_code(
                format!("Unknown backend: {}", backend_str),
                PcaiErrorCode::InvalidInput,
            );
            return PcaiErrorCode::InvalidInput as i32;
        }
    };

    // Initialize global state and create backend
    #[cfg(any(feature = "llamacpp", feature = "mistralrs-backend"))]
    {
        let state = init_global_state();
        let mut guard = match state.lock() {
            Ok(g) => g,
            Err(e) => {
                set_last_error_with_code(
                    format!("Failed to lock state: {}", e),
                    PcaiErrorCode::BackendError,
                );
                return PcaiErrorCode::BackendError as i32;
            }
        };

        // Create backend
        let backend = match backend_type.create() {
            Ok(b) => b,
            Err(e) => {
                set_last_error_with_code(
                    format!("Failed to create backend: {}", e),
                    PcaiErrorCode::BackendError,
                );
                return PcaiErrorCode::BackendError as i32;
            }
        };

        guard.backend = Some(backend);
        tracing::info!("Initialized backend: {}", backend_str);

        PcaiErrorCode::Success as i32
    }
}

/// Load a model into the active backend
///
/// # Arguments
/// * `model_path` - Path to the model file (GGUF or SafeTensors)
/// * `gpu_layers` - Number of layers to offload to GPU (-1 = all, 0 = CPU only)
///
/// # Returns
/// * 0 on success
/// * -1 on error (check pcai_last_error)
///
/// # Safety
/// * `model_path` must be a valid null-terminated C string
/// * Must call pcai_init first
#[no_mangle]
pub extern "C" fn pcai_load_model(model_path: *const c_char, gpu_layers: i32) -> i32 {
    clear_last_error();

    // Parse model path
    let path_str = match unsafe { c_str_from_ptr(model_path) } {
        Ok(s) => s.to_string(),
        Err(e) => {
            set_last_error_with_code(
                format!("Invalid model path: {}", e),
                PcaiErrorCode::InvalidInput,
            );
            return PcaiErrorCode::InvalidInput as i32;
        }
    };

    // Get global state
    let state = init_global_state();
    let mut guard = match state.lock() {
        Ok(g) => g,
        Err(e) => {
            set_last_error_with_code(
                format!("Failed to lock state: {}", e),
                PcaiErrorCode::BackendError,
            );
            return PcaiErrorCode::BackendError as i32;
        }
    };

    // For llamacpp backend, we need to recreate it with the right gpu_layers config
    // This is a limitation of the current backend design - do this before loading
    #[cfg(feature = "llamacpp")]
    {
        if let Some(backend) = guard.backend.as_ref() {
            if backend.backend_name() == "llama.cpp" {
                use crate::backends::llamacpp::LlamaCppBackend;

                let n_gpu_layers = if gpu_layers < 0 {
                    u32::MAX // All layers
                } else {
                    gpu_layers as u32
                };

                let new_backend = LlamaCppBackend::with_config(n_gpu_layers, 8192, 2048);
                guard.backend = Some(Box::new(new_backend));
            }
        }
    }
    #[cfg(not(feature = "llamacpp"))]
    {
        let _ = gpu_layers; // Suppress unused warning
    }

    // Check backend exists
    if guard.backend.is_none() {
        set_last_error_with_code(
            "Backend not initialized. Call pcai_init first.",
            PcaiErrorCode::NotInitialized,
        );
        return PcaiErrorCode::NotInitialized as i32;
    }

    // Load the model (async operation, block on runtime)
    // Split mutable reference to satisfy borrow checker
    let GlobalState { runtime, backend } = &mut *guard;

    if let Some(backend) = backend {
        let result = runtime.block_on(async {
            backend.load_model(&path_str).await
        });

        match result {
            Ok(_) => {
                tracing::info!("Model loaded: {}", path_str);
                PcaiErrorCode::Success as i32
            }
            Err(e) => {
                // Classify error type
                let error_code = if e.to_string().contains("not found")
                    || e.to_string().contains("No such file")
                    || e.to_string().contains("cannot open")
                {
                    PcaiErrorCode::IoError
                } else {
                    PcaiErrorCode::BackendError
                };
                set_last_error_with_code(format!("Failed to load model: {}", e), error_code);
                error_code as i32
            }
        }
    } else {
        set_last_error_with_code(
            "Backend not initialized. Call pcai_init first.",
            PcaiErrorCode::NotInitialized,
        );
        PcaiErrorCode::NotInitialized as i32
    }
}

/// Generate text from a prompt
///
/// # Arguments
/// * `prompt` - Input text prompt (max 100KB)
/// * `max_tokens` - Maximum tokens to generate (0 = default 512)
/// * `temperature` - Sampling temperature (0.0 = greedy, 1.0 = creative)
///
/// # Returns
/// * Pointer to generated text (caller must free with pcai_free_string)
/// * null on error (check pcai_last_error)
///
/// # Safety
/// * `prompt` must be a valid null-terminated C string
/// * Caller must free the returned string with pcai_free_string
/// * Must call pcai_load_model first
#[no_mangle]
pub extern "C" fn pcai_generate(
    prompt: *const c_char,
    max_tokens: u32,
    temperature: f32,
) -> *mut c_char {
    clear_last_error();

    // Parse prompt
    let prompt_str = match unsafe { c_str_from_ptr(prompt) } {
        Ok(s) => s,
        Err(e) => {
            set_last_error_with_code(format!("Invalid prompt: {}", e), PcaiErrorCode::InvalidInput);
            return std::ptr::null_mut();
        }
    };

    // Validate prompt length (100KB max to prevent DoS)
    const MAX_PROMPT_SIZE: usize = 100 * 1024; // 100KB
    if prompt_str.len() > MAX_PROMPT_SIZE {
        set_last_error_with_code(
            format!(
                "Prompt too large: {} bytes (max {})",
                prompt_str.len(),
                MAX_PROMPT_SIZE
            ),
            PcaiErrorCode::InvalidInput,
        );
        return std::ptr::null_mut();
    }

    // Get global state
    let state = init_global_state();
    let guard = match state.lock() {
        Ok(g) => g,
        Err(e) => {
            set_last_error_with_code(
                format!("Failed to lock state: {}", e),
                PcaiErrorCode::BackendError,
            );
            return std::ptr::null_mut();
        }
    };

    // Check backend exists
    let backend = match guard.backend.as_ref() {
        Some(b) => b,
        None => {
            set_last_error_with_code(
                "Backend not initialized. Call pcai_init first.",
                PcaiErrorCode::NotInitialized,
            );
            return std::ptr::null_mut();
        }
    };

    // Check model loaded
    if !backend.is_loaded() {
        set_last_error_with_code(
            "Model not loaded. Call pcai_load_model first.",
            PcaiErrorCode::ModelNotLoaded,
        );
        return std::ptr::null_mut();
    }

    // Build request
    let request = GenerateRequest {
        prompt: prompt_str.to_string(),
        max_tokens: if max_tokens > 0 {
            Some(max_tokens as usize)
        } else {
            None
        },
        temperature: if temperature > 0.0 {
            Some(temperature)
        } else {
            None
        },
        top_p: None,
        stop: vec![],
    };

    // Generate (async operation, block on runtime)
    let result = guard.runtime.block_on(async {
        backend.generate(request).await
    });

    match result {
        Ok(response) => {
            tracing::debug!("Generated {} tokens", response.tokens_generated);

            // Convert to C string
            match CString::new(response.text) {
                Ok(c_str) => c_str.into_raw(),
                Err(e) => {
                    set_last_error_with_code(
                        format!("Failed to create C string: {}", e),
                        PcaiErrorCode::BackendError,
                    );
                    std::ptr::null_mut()
                }
            }
        }
        Err(e) => {
            set_last_error_with_code(
                format!("Generation failed: {}", e),
                PcaiErrorCode::BackendError,
            );
            std::ptr::null_mut()
        }
    }
}

/// Callback function type for streaming generation
///
/// # Arguments
/// * `token` - Generated token text (null-terminated C string)
/// * `user_data` - User-provided data pointer
///
/// # Safety
/// * Callback must not panic
/// * Callback must not call back into pcai functions
pub type TokenCallback = extern "C" fn(token: *const c_char, user_data: *mut c_void);

/// Generate text with streaming callback
///
/// # Arguments
/// * `prompt` - Input text prompt
/// * `max_tokens` - Maximum tokens to generate (0 = default 512)
/// * `temperature` - Sampling temperature (0.0 = greedy, 1.0 = creative)
/// * `callback` - Function to call for each generated token
/// * `user_data` - Pointer to pass to callback
///
/// # Returns
/// * 0 on success
/// * -1 on error (check pcai_last_error)
///
/// # Safety
/// * `prompt` must be a valid null-terminated C string
/// * `callback` must be a valid function pointer
/// * `user_data` can be null
/// * Callback must not call back into pcai functions
/// * Must call pcai_load_model first
#[no_mangle]
#[allow(unused_variables)] // Some parameters unused without llamacpp feature
pub extern "C" fn pcai_generate_streaming(
    prompt: *const c_char,
    max_tokens: u32,
    temperature: f32,
    callback: TokenCallback,
    user_data: *mut c_void,
) -> i32 {
    clear_last_error();

    // Parse prompt
    let prompt_str = match unsafe { c_str_from_ptr(prompt) } {
        Ok(s) => s,
        Err(e) => {
            set_last_error_with_code(
                format!("Invalid prompt: {}", e),
                PcaiErrorCode::InvalidInput,
            );
            return PcaiErrorCode::InvalidInput as i32;
        }
    };

    // Validate prompt length (100KB max to prevent DoS)
    const MAX_PROMPT_SIZE: usize = 100 * 1024; // 100KB
    if prompt_str.len() > MAX_PROMPT_SIZE {
        set_last_error_with_code(
            format!(
                "Prompt too large: {} bytes (max {})",
                prompt_str.len(),
                MAX_PROMPT_SIZE
            ),
            PcaiErrorCode::InvalidInput,
        );
        return PcaiErrorCode::InvalidInput as i32;
    }

    let prompt_str = prompt_str.to_string();

    // Get global state
    let state = init_global_state();
    let guard = match state.lock() {
        Ok(g) => g,
        Err(e) => {
            set_last_error_with_code(
                format!("Failed to lock state: {}", e),
                PcaiErrorCode::BackendError,
            );
            return PcaiErrorCode::BackendError as i32;
        }
    };

    // Check backend exists
    let backend = match guard.backend.as_ref() {
        Some(b) => b,
        None => {
            set_last_error_with_code(
                "Backend not initialized. Call pcai_init first.",
                PcaiErrorCode::NotInitialized,
            );
            return PcaiErrorCode::NotInitialized as i32;
        }
    };

    // Check model loaded
    if !backend.is_loaded() {
        set_last_error_with_code(
            "Model not loaded. Call pcai_load_model first.",
            PcaiErrorCode::ModelNotLoaded,
        );
        return PcaiErrorCode::ModelNotLoaded as i32;
    }

    // Build request
    let request = GenerateRequest {
        prompt: prompt_str,
        max_tokens: if max_tokens > 0 {
            Some(max_tokens as usize)
        } else {
            None
        },
        temperature: if temperature > 0.0 {
            Some(temperature)
        } else {
            None
        },
        top_p: None,
        stop: vec![],
    };

    // Generate with streaming (backend-specific implementation)
    // SAFETY: We convert the pointer to usize to make it Send-safe across the async boundary.
    // This is safe because we're using block_on (not spawning a separate thread) and the
    // callback/user_data are assumed valid for the duration of this call.
    let user_data_addr = user_data as usize;
    let result = guard.runtime.block_on(async move {
        let mut bridge = move |token: String| {
            if let Ok(c_token) = CString::new(token) {
                // SAFETY: We're reconstructing the pointer from the address we saved earlier.
                // The caller must ensure the pointer remains valid for the duration of this call.
                let user_data_ptr = user_data_addr as *mut c_void;
                callback(c_token.as_ptr(), user_data_ptr);
            }
        };
        backend.generate_streaming(request, &mut bridge).await
    });

    match result {
        Ok(response) => {
            tracing::debug!("Streaming completed: {} tokens", response.tokens_generated);
            PcaiErrorCode::Success as i32
        }
        Err(e) => {
            set_last_error_with_code(
                format!("Streaming generation failed: {}", e),
                PcaiErrorCode::BackendError,
            );
            PcaiErrorCode::BackendError as i32
        }
    }
}

/// Free a string returned by pcai_generate
///
/// # Arguments
/// * `s` - String pointer returned by pcai_generate
///
/// # Safety
/// * `s` must be a pointer returned by pcai_generate or null
/// * Must not be called twice on the same pointer
#[no_mangle]
pub extern "C" fn pcai_free_string(s: *mut c_char) {
    if s.is_null() {
        return;
    }

    unsafe {
        let _ = CString::from_raw(s);
    }
}

/// Shutdown the inference engine and free resources
///
/// # Safety
/// * After calling this, all other functions will fail until pcai_init is called again
#[no_mangle]
pub extern "C" fn pcai_shutdown() {
    clear_last_error();

    let state = init_global_state();
    let mut guard = match state.lock() {
        Ok(g) => g,
        Err(e) => {
            set_last_error_with_code(
                format!("Failed to lock state: {}", e),
                PcaiErrorCode::BackendError,
            );
            return;
        }
    };

    if let Some(mut backend) = guard.backend.take() {
        // Unload model
        let _ = guard.runtime.block_on(async {
            backend.unload_model().await
        });

        tracing::info!("Backend shutdown");
    }
}

/// Get the last error message for the current thread
///
/// # Returns
/// * Pointer to error message (do NOT free)
/// * null if no error
///
/// # Safety
/// * Returned pointer is valid until the next call to any pcai function
/// * Do NOT call pcai_free_string on this pointer
#[no_mangle]
pub extern "C" fn pcai_last_error() -> *const c_char {
    LAST_ERROR.with(|e| {
        let err_ref = e.borrow();
        match err_ref.as_ref() {
            Some(err) => {
                // SAFETY: We need to return a stable pointer that lives beyond this function.
                // We leak a CString to ensure it stays alive. This is acceptable for error
                // messages as they are infrequent and small.
                match CString::new(err.as_str()) {
                    Ok(c_str) => {
                        let ptr = c_str.as_ptr();
                        std::mem::forget(c_str); // Leak to keep alive
                        ptr
                    }
                    Err(_) => std::ptr::null(),
                }
            }
            None => std::ptr::null(),
        }
    })
}

/// Get the error code for the last error
///
/// # Returns
/// * Error code as i32 (see PcaiErrorCode enum)
/// * 0 if no error (Success)
#[no_mangle]
pub extern "C" fn pcai_last_error_code() -> i32 {
    LAST_ERROR_CODE.with(|c| *c.borrow() as i32)
}

/// Get the crate version string
///
/// # Returns
/// * Pointer to version string (do NOT free)
///
/// # Safety
/// * Returned pointer is valid for the lifetime of the program
/// * Do NOT call pcai_free_string on this pointer
#[no_mangle]
pub extern "C" fn pcai_version() -> *const c_char {
    static VERSION: &str = concat!(env!("CARGO_PKG_VERSION"), "\0");
    VERSION.as_ptr() as *const c_char
}

// ============================================================================
// Tests
// ============================================================================

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_error_code_enum() {
        // Test enum values
        assert_eq!(PcaiErrorCode::Success as i32, 0);
        assert_eq!(PcaiErrorCode::NotInitialized as i32, -1);
        assert_eq!(PcaiErrorCode::ModelNotLoaded as i32, -2);
        assert_eq!(PcaiErrorCode::InvalidInput as i32, -3);
        assert_eq!(PcaiErrorCode::BackendError as i32, -4);
        assert_eq!(PcaiErrorCode::IoError as i32, -5);
        assert_eq!(PcaiErrorCode::Unknown as i32, -99);

        // Test from_i32
        assert_eq!(PcaiErrorCode::from_i32(0), PcaiErrorCode::Success);
        assert_eq!(PcaiErrorCode::from_i32(-1), PcaiErrorCode::NotInitialized);
        assert_eq!(PcaiErrorCode::from_i32(-2), PcaiErrorCode::ModelNotLoaded);
        assert_eq!(PcaiErrorCode::from_i32(-3), PcaiErrorCode::InvalidInput);
        assert_eq!(PcaiErrorCode::from_i32(-4), PcaiErrorCode::BackendError);
        assert_eq!(PcaiErrorCode::from_i32(-5), PcaiErrorCode::IoError);
        assert_eq!(PcaiErrorCode::from_i32(-99), PcaiErrorCode::Unknown);
        assert_eq!(PcaiErrorCode::from_i32(-1000), PcaiErrorCode::Unknown);
    }

    #[test]
    fn test_version() {
        let version_ptr = pcai_version();
        assert!(!version_ptr.is_null());

        let version_str = unsafe { CStr::from_ptr(version_ptr) };
        let version = version_str.to_str().unwrap();

        // Version should be non-empty and match CARGO_PKG_VERSION
        assert!(!version.is_empty());
        assert_eq!(version, env!("CARGO_PKG_VERSION"));
    }

    #[test]
    fn test_error_storage() {
        clear_last_error();
        assert!(pcai_last_error().is_null());
        assert_eq!(pcai_last_error_code(), PcaiErrorCode::Success as i32);

        set_last_error("test error");
        let err_ptr = pcai_last_error();
        assert!(!err_ptr.is_null());

        let err_str = unsafe { CStr::from_ptr(err_ptr) };
        assert_eq!(err_str.to_str().unwrap(), "test error");
        assert_eq!(pcai_last_error_code(), PcaiErrorCode::Unknown as i32);

        clear_last_error();
        assert_eq!(pcai_last_error_code(), PcaiErrorCode::Success as i32);
    }

    #[test]
    fn test_error_code_storage() {
        clear_last_error();
        assert_eq!(pcai_last_error_code(), PcaiErrorCode::Success as i32);

        set_last_error_with_code("test error", PcaiErrorCode::InvalidInput);
        assert_eq!(pcai_last_error_code(), PcaiErrorCode::InvalidInput as i32);

        set_last_error_with_code("backend error", PcaiErrorCode::BackendError);
        assert_eq!(pcai_last_error_code(), PcaiErrorCode::BackendError as i32);

        clear_last_error();
        assert_eq!(pcai_last_error_code(), PcaiErrorCode::Success as i32);
    }

    #[test]
    fn test_init_null_backend() {
        let result = pcai_init(std::ptr::null());
        assert_eq!(result, PcaiErrorCode::InvalidInput as i32);
        assert!(!pcai_last_error().is_null());
        assert_eq!(pcai_last_error_code(), PcaiErrorCode::InvalidInput as i32);
    }

    #[test]
    fn test_generate_before_init() {
        // Reset state by calling shutdown
        pcai_shutdown();

        let prompt = CString::new("test").unwrap();
        let result = pcai_generate(prompt.as_ptr(), 10, 0.7);
        assert!(result.is_null());
        assert!(!pcai_last_error().is_null());
        assert_eq!(pcai_last_error_code(), PcaiErrorCode::NotInitialized as i32);
    }

    #[test]
    fn test_prompt_length_validation() {
        pcai_shutdown();

        // Create a prompt larger than 100KB
        let large_prompt = "x".repeat(101 * 1024);
        let prompt_cstr = CString::new(large_prompt).unwrap();

        let result = pcai_generate(prompt_cstr.as_ptr(), 10, 0.7);
        assert!(result.is_null());

        let err_ptr = pcai_last_error();
        assert!(!err_ptr.is_null());

        let err_str = unsafe { CStr::from_ptr(err_ptr) };
        let err_text = err_str.to_str().unwrap();

        // Error should mention prompt size
        assert!(
            err_text.contains("too large") || err_text.contains("Prompt"),
            "Error should mention prompt size: {}",
            err_text
        );
        assert_eq!(pcai_last_error_code(), PcaiErrorCode::InvalidInput as i32);
    }
}
