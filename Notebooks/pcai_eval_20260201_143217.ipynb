{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PC-AI Comprehensive LLM Evaluation Suite\n",
        "\n",
        "This notebook drives a parameter sweep across PC-AI backends, captures system resource telemetry, and graphs performance/quality metrics. It also includes optional hooks for standard LLM benchmarks via `lm-evaluation-harness`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, json, time, datetime, subprocess, sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PROJECT_ROOT = Path(r'C:\\\\Users\\\\david\\\\PC_AI')\n",
        "REPORTS_DIR = PROJECT_ROOT / 'Reports' / 'llm-eval' / datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print('Reports:', REPORTS_DIR)\n",
        "\n",
        "def find_powershell():\n",
        "    for exe in ['powershell', 'pwsh']:\n",
        "        try:\n",
        "            subprocess.run([exe, '-NoProfile', '-Command', '$PSVersionTable.PSVersion'], check=True, capture_output=True, text=True)\n",
        "            return exe\n",
        "        except Exception:\n",
        "            continue\n",
        "    raise RuntimeError('PowerShell not found')\n",
        "\n",
        "POWERSHELL = find_powershell()\n",
        "print('Using PowerShell:', POWERSHELL)\n",
        "\n",
        "def run_ps(cmd, check=True):\n",
        "    result = subprocess.run([POWERSHELL, '-NoProfile', '-ExecutionPolicy', 'Bypass', '-Command', cmd],\n",
        "                            capture_output=True, text=True)\n",
        "    if check and result.returncode != 0:\n",
        "        raise RuntimeError(f'PowerShell failed: {result.stderr}\\nSTDOUT:\\n{result.stdout}')\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load PC-AI modules\n",
        "This ensures the evaluation and performance modules are available for native and compiled-backend tests.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ps = f'''\n",
        "Import-Module '{PROJECT_ROOT / 'Modules' / 'PcaiInference.psd1'}' -Force -ErrorAction SilentlyContinue\n",
        "Import-Module '{PROJECT_ROOT / 'Modules' / 'PC-AI.Evaluation' / 'PC-AI.Evaluation.psd1'}' -Force\n",
        "Import-Module '{PROJECT_ROOT / 'Modules' / 'PC-AI.Performance' / 'PC-AI.Performance.psd1'}' -Force\n",
        "Get-PcaiInferenceStatus | Format-List\n",
        "'''\n",
        "print(run_ps(ps).stdout)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model discovery\n",
        "We auto-detect a test model. For llama.cpp we need a `.gguf`. For mistral.rs, a SafeTensors model is acceptable. Set `PCAI_TEST_MODEL` to override.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def find_model_path():\n",
        "    env_path = os.environ.get('PCAI_TEST_MODEL')\n",
        "    if env_path and Path(env_path).exists():\n",
        "        return Path(env_path)\n",
        "\n",
        "    # Prefer local repo Models\n",
        "    models_dir = PROJECT_ROOT / 'Models'\n",
        "    gguf = list(models_dir.rglob('*.gguf'))\n",
        "    if gguf:\n",
        "        return gguf[0]\n",
        "\n",
        "    # Try functiongemma safetensors\n",
        "    gemma = models_dir / 'functiongemma-270m-it' / 'model.safetensors'\n",
        "    if gemma.exists():\n",
        "        return gemma\n",
        "\n",
        "    # Ollama / LM Studio caches\n",
        "    lm = Path(os.environ.get('LOCALAPPDATA','')) / 'lm-studio' / 'models'\n",
        "    if lm.exists():\n",
        "        candidates = list(lm.rglob('*.gguf'))\n",
        "        if candidates:\n",
        "            return candidates[0]\n",
        "\n",
        "    ollama = Path.home() / '.ollama' / 'models' / 'blobs'\n",
        "    if ollama.exists():\n",
        "        candidates = list(ollama.rglob('*.gguf'))\n",
        "        if candidates:\n",
        "            return candidates[0]\n",
        "\n",
        "    return None\n",
        "\n",
        "MODEL_PATH = find_model_path()\n",
        "print('MODEL_PATH:', MODEL_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameter grid\n",
        "Adjust the sweep parameters here to control run time and coverage.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if MODEL_PATH is None:\n",
        "    raise RuntimeError('No model found. Set PCAI_TEST_MODEL or add a gguf/safetensors model.')\n",
        "\n",
        "backends = []\n",
        "if str(MODEL_PATH).lower().endswith('.gguf'):\n",
        "    backends.append('llamacpp-bin')\n",
        "backends.append('mistralrs-bin')\n",
        "\n",
        "temperatures = [0.1, 0.7, 1.0]\n",
        "max_tokens = [64, 128, 256]\n",
        "gpu_layers = [-1, 0]\n",
        "datasets = ['diagnostic', 'general', 'safety']\n",
        "max_test_cases = 5  # keep runs tight; increase for deeper evals\n",
        "\n",
        "print('Backends:', backends)\n",
        "print('Datasets:', datasets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper: system resource monitoring\n",
        "Uses `Watch-SystemResources` from PC-AI.Performance in `Object` mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def start_resource_monitor(output_csv: Path, duration_s: int, refresh_s: int = 1):\n",
        "    ps = f'''\n",
        "Import-Module '{PROJECT_ROOT / 'Modules' / 'PC-AI.Performance' / 'PC-AI.Performance.psd1'}' -Force\n",
        "Watch-SystemResources -Duration {duration_s} -RefreshInterval {refresh_s} -OutputMode Object | Export-Csv -NoTypeInformation -Path '{output_csv}'\n",
        "'''\n",
        "    return subprocess.Popen([POWERSHELL, '-NoProfile', '-ExecutionPolicy', 'Bypass', '-Command', ps],\n",
        "                            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run evaluation grid\n",
        "Each run generates JSON metrics and a CSV with system telemetry.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results = []\n",
        "for backend in backends:\n",
        "    for dataset in datasets:\n",
        "        for temp in temperatures:\n",
        "            for max_tok in max_tokens:\n",
        "                for gpu in gpu_layers:\n",
        "                    run_id = f\"{backend}_{dataset}_t{temp}_mt{max_tok}_gpu{gpu}\".replace('.', 'p')\n",
        "                    out_json = REPORTS_DIR / f\"{run_id}.json\"\n",
        "                    out_csv = REPORTS_DIR / f\"{run_id}_resources.csv\"\n",
        "\n",
        "                    # Estimate duration budget for monitoring\n",
        "                    duration = max(30, max_test_cases * 6)\n",
        "                    monitor = start_resource_monitor(out_csv, duration_s=duration, refresh_s=1)\n",
        "\n",
        "                    cmd = f\"\"\"& '{PROJECT_ROOT / 'Tests' / 'Evaluation' / 'Invoke-InferenceEvaluation.ps1'}' `\n",
        "    -Backend {backend} `\n",
        "    -ModelPath '{MODEL_PATH}' `\n",
        "    -Dataset {dataset} `\n",
        "    -MaxTokens {max_tok} `\n",
        "    -Temperature {temp} `\n",
        "    -GpuLayers {gpu} `\n",
        "    -MaxTestCases {max_test_cases} `\n",
        "    -OutputPath '{out_json}'\n",
        "\"\"\"\n",
        "                    print('Running:', run_id)\n",
        "                    ps_result = run_ps(cmd, check=False)\n",
        "\n",
        "                    # Wait for monitor to complete\n",
        "                    try:\n",
        "                        monitor.wait(timeout=duration + 10)\n",
        "                    except subprocess.TimeoutExpired:\n",
        "                        monitor.kill()\n",
        "\n",
        "                    if ps_result.returncode != 0:\n",
        "                        print('Run failed:', run_id, ps_result.stderr)\n",
        "                        continue\n",
        "\n",
        "                    if not out_json.exists():\n",
        "                        print('Missing output:', out_json)\n",
        "                        continue\n",
        "\n",
        "                    payload = json.loads(out_json.read_text())\n",
        "                    # Extract summary for this backend\n",
        "                    summary = None\n",
        "                    for item in payload.get('Results', []):\n",
        "                        if item.get('Key') == backend:\n",
        "                            summary = item.get('Value')\n",
        "                            break\n",
        "                    if summary is None:\n",
        "                        summary = {}\n",
        "\n",
        "                    results.append({\n",
        "                        'run_id': run_id,\n",
        "                        'backend': backend,\n",
        "                        'dataset': dataset,\n",
        "                        'temperature': temp,\n",
        "                        'max_tokens': max_tok,\n",
        "                        'gpu_layers': gpu,\n",
        "                        'pass_rate': summary.get('PassRate'),\n",
        "                        'avg_score': summary.get('AverageScore'),\n",
        "                        'avg_latency_ms': summary.get('AverageLatency'),\n",
        "                        'resources_csv': str(out_csv)\n",
        "                    })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "display(df.head())\n",
        "df.to_csv(REPORTS_DIR / 'summary.csv', index=False)\n",
        "print('Saved summary:', REPORTS_DIR / 'summary.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot performance + quality metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if df.empty:\n",
        "    raise RuntimeError('No results collected. Check logs in Reports for failures.')\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "for backend in df['backend'].unique():\n",
        "    subset = df[df['backend'] == backend]\n",
        "    plt.plot(subset['avg_latency_ms'], label=f'{backend} latency')\n",
        "plt.title('Average Latency per Run')\n",
        "plt.xlabel('Run Index')\n",
        "plt.ylabel('Latency (ms)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "for backend in df['backend'].unique():\n",
        "    subset = df[df['backend'] == backend]\n",
        "    plt.plot(subset['avg_score'], label=f'{backend} score')\n",
        "plt.title('Average Quality Score per Run')\n",
        "plt.xlabel('Run Index')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot system resource telemetry (CPU + memory)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_resources(csv_path):\n",
        "    data = pd.read_csv(csv_path)\n",
        "    if data.empty:\n",
        "        return\n",
        "    fig, ax1 = plt.subplots(figsize=(12, 5))\n",
        "    ax1.plot(data['CpuPercent'], label='CPU %')\n",
        "    ax1.set_ylabel('CPU %')\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(data['MemoryPercent'], color='orange', label='Memory %')\n",
        "    ax2.set_ylabel('Memory %')\n",
        "    ax1.set_title(Path(csv_path).name)\n",
        "    fig.legend(loc='upper right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot first few resource traces\n",
        "for csv_path in df['resources_csv'].head(3):\n",
        "    plot_resources(csv_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Standard LLM benchmark suite (lm-evaluation-harness)\n",
        "This step uses `lm-evaluation-harness` (EleutherAI) to run short subsets of common tasks (ARC Easy, HellaSwag, TruthfulQA). It is optional and will skip if installation fails.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def ensure_lm_eval():\n",
        "    try:\n",
        "        import lm_eval  # noqa: F401\n",
        "        return True\n",
        "    except Exception:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'lm-eval[openai]>=0.4.4'])\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print('lm-eval install failed:', e)\n",
        "            return False\n",
        "\n",
        "if ensure_lm_eval():\n",
        "    tasks = 'arc_easy,hellaswag,truthfulqa_mc'\n",
        "    base_url = 'http://127.0.0.1:8080/v1'\n",
        "    cmd = [\n",
        "        sys.executable, '-m', 'lm_eval',\n",
        "        '--model', 'openai-chat-completions',\n",
        "        '--model_args', f'model=pcai-inference,api_base={base_url},api_key=sk-local',\n",
        "        '--tasks', tasks,\n",
        "        '--limit', '5',\n",
        "        '--output_path', str(REPORTS_DIR / 'lm_eval_results.json')\n",
        "    ]\n",
        "    print('Running lm-eval:', ' '.join(cmd))\n",
        "    subprocess.run(cmd, check=False)\n",
        "else:\n",
        "    print('lm-eval not available; skipping standard benchmark suite.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimization recommendation\n",
        "We compute a simple composite score to pick a high-performance configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_clean = df.dropna(subset=['avg_score', 'avg_latency_ms']).copy()\n",
        "df_clean['score_norm'] = df_clean['avg_score']\n",
        "df_clean['lat_norm'] = 1.0 - (df_clean['avg_latency_ms'] / df_clean['avg_latency_ms'].max())\n",
        "df_clean['composite'] = (df_clean['score_norm'] * 0.6) + (df_clean['lat_norm'] * 0.4)\n",
        "best = df_clean.sort_values('composite', ascending=False).head(5)\n",
        "display(best)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}